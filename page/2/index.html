<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Hexo</title>
  <meta name="description" content="计算机视觉，人工智能">
<meta property="og:type" content="website">
<meta property="og:title" content="Multi-Dimensional Computer Vision Team">
<meta property="og:url" content="http://sues-vision.github.io/page/2/index.html">
<meta property="og:site_name" content="Multi-Dimensional Computer Vision Team">
<meta property="og:description" content="计算机视觉，人工智能">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Atiuo">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://sues-vision.github.io/page/2/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Multi-Dimensional Computer Vision Team" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/1.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/sues-vision" target="_blank">
          <img class="img-circle img-rotate" src="/img/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">多维度计算机视觉团队</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Multi-Dimensional Computer Vision Team</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shanghai, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-papers">
          <a href="/2017/12/31/03-paperlist">
            
            <span class="menu-title">论文</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/2017/12/31/02-projects">
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-reward">
          <a href="/reward">
            
            <span class="menu-title">获奖</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-patents">
          <a href="/Patents">
            
            <span class="menu-title">专利</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-graduate">
          <a href="/graduate">
            
            <span class="menu-title">毕业生去向</span>
          </a>
        </li>
        
      </ul>
      
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/%E4%B8%89%E7%BB%B4-3D/" style="font-size: 13.75px;">三维 3D</a> <a href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB-Action-Recognition/" style="font-size: 13px;">动作识别 Action Recognition</a> <a href="/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F-Medical-Image/" style="font-size: 13.25px;">医学图像 Medical Image</a> <a href="/tags/%E5%8D%95%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA-Single-object-Tracking/" style="font-size: 13px;">单目标跟踪 Single-object Tracking</a> <a href="/tags/%E5%90%8C%E6%AD%A5%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%9C%B0%E5%9B%BE%E7%BB%98%E5%88%B6-SLAM/" style="font-size: 13.25px;">同步定位与地图绘制 SLAM</a> <a href="/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-GNN/" style="font-size: 13px;">图神经网络 GNN</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81-CLIP/" style="font-size: 13px;">多模态 CLIP</a> <a href="/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA-MOT/" style="font-size: 13.25px;">多目标跟踪 MOT</a> <a href="/tags/%E5%AD%A6%E6%9C%AF%E4%BA%A4%E6%B5%81-Academic-Activities/" style="font-size: 13px;">学术交流 Academic Activities</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88-Data-Fusion/" style="font-size: 13.25px;">数据融合 Data Fusion</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Unsupervised-Learning/" style="font-size: 13px;">无监督学习 Unsupervised Learning</a> <a href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Attention/" style="font-size: 13.75px;">注意力机制 Attention</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Object-Detection/" style="font-size: 13.5px;">目标检测 Object Detection</a> <a href="/tags/%E8%87%AA%E4%B8%BB%E8%BF%90%E5%8A%A8-ego-motion/" style="font-size: 13px;">自主运动 ego-motion</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6-Automatic-Driving/" style="font-size: 13px;">自动驾驶 Automatic Driving</a> <a href="/tags/%E8%A1%8C%E4%BA%BA%E5%88%86%E5%89%B2-Person-Segmentation/" style="font-size: 13.25px;">行人分割 Person Segmentation</a> <a href="/tags/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB-Person-re-identification/" style="font-size: 13.5px;">行人重识别 Person re-identification</a> <a href="/tags/%E8%A3%82%E7%BC%9D%E6%A3%80%E6%B5%8B-Crack-Detection/" style="font-size: 13px;">裂缝检测 Crack Detection</a> <a href="/tags/%E8%AE%BA%E6%96%87-Article/" style="font-size: 14px;">论文 Article</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">五月 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">五月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
              </p>
              <p class="item-title">
                <a href="/2024/05/14/paper-2023-MDPI-Symmetry/" class="title">Fan L，Chen W，Jiang X Cross-Correlation Fusion Graph Convolution-Based Object Tracking，*Symmetry* 2023</a>
              </p>
              <p class="item-date">
                <time datetime="2024-05-14T05:36:20.000Z" itemprop="datePublished">2024-05-14</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
              </p>
              <p class="item-title">
                <a href="/2024/05/14/paper-2023-IEEE-TSMC/" class="title">Xiaoyan Jiang, J N Hwang and Z Fang, &#34;A Multiscale Coarse-to-Fine Human Pose Estimation Network With Hard Keypoint Mining&#34; in IEEE Transactions on Systems, Man, and Cybernetics:Systems, March 2024</a>
              </p>
              <p class="item-date">
                <time datetime="2024-05-14T02:25:39.000Z" itemprop="datePublished">2024-05-14</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
              </p>
              <p class="item-title">
                <a href="/2024/05/11/paper-2023-ictee/" class="title">Kunlun Xue, Xiaoyan Jiang, Zhichao Chen“A SLAM Method Based on ORB-SLAM3 Which Mixed GNSS Data” International Conference on Information Technologies and Electrical Engineering</a>
              </p>
              <p class="item-date">
                <time datetime="2024-05-11T07:52:27.000Z" itemprop="datePublished">2024-05-11</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
              </p>
              <p class="item-title">
                <a href="/2024/05/07/paper-2023-IEEE/" class="title">Wenwen Zheng,Xiaoyan Jiang, Zhijun Fang etc, &#34;TV-Net:A Structure-Level Feature Fusion Network Based on Tensor Voting for Road Crack Segmentation&#34; in IEEE Transactions on Intelligent Transportation Systems, June 2024</a>
              </p>
              <p class="item-date">
                <time datetime="2024-05-07T07:51:43.000Z" itemprop="datePublished">2024-05-07</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
              </p>
              <p class="item-title">
                <a href="/2024/05/06/paper-2023-PR/" class="title">Baihong Han, Xiaoyan Jiang, Zhijun Fang, Hamido Fujita, Yongbin Gao,F-SCP:An automatic prompt generation method for specific classes based on visual language pre-training models,*Pattern Recognition*,2024</a>
              </p>
              <p class="item-date">
                <time datetime="2024-05-06T09:43:16.000Z" itemprop="datePublished">2024-05-06</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  <main class="main" role="main">
  
  <div class="content article-list">
    
      <article class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/06/paper-2023-PR/">Baihong Han, Xiaoyan Jiang, Zhijun Fang, Hamido Fujita, Yongbin Gao,F-SCP:An automatic prompt generation method for specific classes based on visual language pre-training models,*Pattern Recognition*,2024</a>
    </h1>
  

  </div>
  
  <div class="article-entry text-muted" itemprop="description">
    <p>团队2021级研究生韩柏宏同学的论文“F-SCP: An automatic prompt generation method for specific classes based onvisual language pre-training models”被SCI顶刊<a target="_blank" rel="noopener" href="https://www.springer.com/journal/10489/">《Parttern Recognition》</a> 录用，祝贺！</p>
<p>Abstract:<br>The zero-shot classification performance of large-scale vision-language pre-training models (e.g., CLIP, BLIP and ALIGN) can be enhanced by incorporating a prompt (e.g., “a photo of a [CLASS]”) before the class words. Modifying the prompt slightly can have significant effect on the classification outcomes of these models. Thus, it is crucial to include an appropriate prompt tailored to the classes. However, manual prompt design is labor-intensive and necessitates domain-specific expertise. The CoOp (Context Optimization) converts hand-crafted prompt templates into learnable word vectors to automatically generate prompts, resulting in substantial improvements for CLIP. However, CoOp exhibited significant variation in classification performance across different classes. Although CoOp-CSC (Class-Specific Context) has a separate prompt for each class, only shows some advantages on fine-grained datasets. In this paper, we propose a novel automatic prompt generation method called F-SCP (Filter-based Specific Class Prompt), which distinguishes itself from the CoOp-UC (Unified Context) model and the CoOp-CSC model. Our approach focuses on prompt generation for low-accuracy classes and similar classes. We add the Filter and SCP modules to the prompt generation architecture. The Filter module selects the poorly classified classes, and then reproduce the prompts through the SCP (Specific Class Prompt) module to replace the prompts of specific classes. Experimental results on six multi-domain datasets shows the superiority of our approach over the state-of-the-art methods. Particularly, the improvement in accuracy for the specific classes mentioned above is significant. For instance, compared with CoOp-UC on the OxfordPets dataset, the low-accuracy classes, such as, Class21 and Class26, are improved by 18% and 12%, respectively.</p>
<p><strong>Download:</strong> <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0031320323007938?via=ihub">[官方链接]</a> <a target="_blank" rel="noopener" href="https://github.com/sues-vision/F-SCP">[preprint版本]</a></p>
<p><strong>Keywords:</strong> Multi-modalVision language modelPrompt tuningLarge-scale pre-training model<br><strong>Photos:</strong> </p>
<img src="/2024/05/06/paper-2023-PR/F2.png" class>
  </div>
  
  <p class="article-meta">
    <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2024/05/06/paper-2023-PR/" class="article-date">
	  <time datetime="2024-05-06T09:43:16.000Z" itemprop="datePublished">5月 6</time>
	</a>
</span>
    
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
  </span>

    
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81-CLIP/" rel="tag">多模态 CLIP</a>, <a class="article-tag-link-link" href="/tags/%E8%AE%BA%E6%96%87-Article/" rel="tag">论文 Article</a>
  </span>


    <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2024/05/06/paper-2023-PR/#comments" class="article-comment-link">评论</a></span>
    
  </p>
</article>

    
      <article class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/05/06/paper-2019-ICONIP/">Yang Li and Xiaoyan Jiang ,Jenq-Neng Hwang“Discriminant Feature Learning with Self-Attention for Person Re-Identiﬁcation” ICONIP Meeting</a>
    </h1>
  

  </div>
  
  <p class="article-meta">
    <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2024/05/06/paper-2019-ICONIP/" class="article-date">
	  <time datetime="2024-05-06T07:18:48.842Z" itemprop="datePublished">5月 6</time>
	</a>
</span>
    
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
  </span>

    
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Attention/" rel="tag">注意力机制 Attention</a>, <a class="article-tag-link-link" href="/tags/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB-Person-re-identification/" rel="tag">行人重识别 Person re-identification</a>, <a class="article-tag-link-link" href="/tags/%E8%AE%BA%E6%96%87-Article/" rel="tag">论文 Article</a>
  </span>


    <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2024/05/06/paper-2019-ICONIP/#comments" class="article-comment-link">评论</a></span>
    
  </p>
</article>

    
      <article class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/15/paper-2022-PR/">Yingmao Yao, Xiaoyan Jiang, Hamido Fujita, Zhijun Fang,A sparse graph wavelet convolution neural network for video-based person re-identification,Pattern Recognition,2022</a>
    </h1>
  

  </div>
  
  <p class="article-meta">
    <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2022/04/15/paper-2022-PR/" class="article-date">
	  <time datetime="2022-04-15T08:28:10.000Z" itemprop="datePublished">4月 15</time>
	</a>
</span>
    
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
  </span>

    
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-GNN/" rel="tag">图神经网络 GNN</a>, <a class="article-tag-link-link" href="/tags/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB-Person-re-identification/" rel="tag">行人重识别 Person re-identification</a>, <a class="article-tag-link-link" href="/tags/%E8%AE%BA%E6%96%87-Article/" rel="tag">论文 Article</a>
  </span>


    <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2022/04/15/paper-2022-PR/#comments" class="article-comment-link">评论</a></span>
    
  </p>
</article>

    
      <article class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/05/10/paper-2021-ASC/">Yi Wu, Xiaoyan Jiang, Zhijun Fang, Yongbin Gao, Hamido Fujita,Multi-modal 3D object detection by 2D-guided precision anchor proposal and multi-layer fusion,Applied Soft Computing,2021</a>
    </h1>
  

  </div>
  
  <div class="article-entry text-muted" itemprop="description">
    <p>团队2018级研究生吴益同学的论文“Multi-modal 3D object detection by 2D-guided precision anchor proposal and multi-layer fusion”被SCI期刊<a target="_blank" rel="noopener" href="https://www.journals.elsevier.com/applied-soft-computing/">《Applied Soft Computing》</a> 录用，祝贺！</p>
<p>Abstract:<br>3D object detection, of which the goal is to obtain the 3D spatial structure information of the object, is a challenging topic in many visual perception systems, e.g., autonomous driving, augmented reality, and robot navigation. Most existing region proposal network (RPN) based 3D object detection methods generate anchors in the whole 3D searching space without using semantic information, which leads to the problem of inappropriate anchor size generation. To tackle the issue, we propose a 2D-guided precision anchor generation network (PAG-Net). Specifically speaking, we utilize a mature 2D detector to get 2D bounding boxes and category labels of objects as prior information. Then the 2D bounding boxes are projected into 3D frustum space for more precise and category-adaptive 3D anchors. Furthermore, current feature combination methods are early fusion, late fusion, and deep fusion, which only fuse features from high convolutional layers and ignore the data missing problem of point clouds. To obtain more efficient fusion of RGB images and point clouds features, we propose a multi-layer fusion model, which conducts nonlinear and iterative combinations of features from multiple convolutional layers and merges the global and local features effectively. We encode point cloud with the bird’s eye view (BEV) representation to solve the irregularity of point cloud. Experimental results show that our proposed approach improves the baseline by a large margin and outperforms most of the state-of-the-art methods on the KITTI object detection benchmark.</p>
<p><strong>Download:</strong> <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S1568494621003288">[官方链接]</a> <a href="(https://github.com/sues-vision/pre-print-papers">[preprint版本]</a></p>
<p><strong>Keywords:</strong> 3D object detection, Multi-modal, Autonomous driving, Feature fusion, Point cloud.</p>
<p><strong>Photos:</strong></p>
<img src="/2021/05/10/paper-2021-ASC/fig2.png" class>
  </div>
  
  <p class="article-meta">
    <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2021/05/10/paper-2021-ASC/" class="article-date">
	  <time datetime="2021-05-10T12:42:11.000Z" itemprop="datePublished">5月 10</time>
	</a>
</span>
    
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
  </span>

    
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E4%B8%89%E7%BB%B4-3D/" rel="tag">三维 3D</a>, <a class="article-tag-link-link" href="/tags/%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88-Data-Fusion/" rel="tag">数据融合 Data Fusion</a>, <a class="article-tag-link-link" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Object-Detection/" rel="tag">目标检测 Object Detection</a>, <a class="article-tag-link-link" href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6-Automatic-Driving/" rel="tag">自动驾驶 Automatic Driving</a>, <a class="article-tag-link-link" href="/tags/%E8%AE%BA%E6%96%87-Article/" rel="tag">论文 Article</a>
  </span>


    <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2021/05/10/paper-2021-ASC/#comments" class="article-comment-link">评论</a></span>
    
  </p>
</article>

    
      <article class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/03/01/paper-2021-AI/">Jiang, G., Jiang, X., Fang, Z. et al. An efficient attention module for 3d convolutional neural networks in action recognition. Appl Intell</a>
    </h1>
  

  </div>
  
  <div class="article-entry text-muted" itemprop="description">
    <p>团队2018级研究生蒋光好同学的论文“An efficient attention module for 3d convolutional neural networks in action recognition”被SCI期刊<a target="_blank" rel="noopener" href="https://www.springer.com/journal/10489/">《Applied Intelligence》</a> 录用，祝贺！</p>
<p>Abstract:<br>Due to illumination changes, varying postures, and occlusion, accurately recognizing actions in videos is still a challenging task. A three-dimensional convolutional neural network (3D CNN), which can simultaneously extract spatio-temporal features from sequences, is one of the mainstream models for action recognition. However, most of the existing 3D CNN models ignore the importance of individual frames and spatial regions when recognizing actions. To address this problem, we propose an efficient attention module (EAM) that contains two sub-modules, that is, a spatial efficient attention module (EAM-S) and a temporal efficient attention module (EAM-T). Specifically, without dimensionality reduction, EAM-S concentrates on mining category-based correlation by local cross-channel interaction and assigns high weights to important image regions, while EAM-T estimates the importance score of different frames by cross-frame interaction between each frame and its neighbors. The proposed EAM module is lightweight yet effective, and it can be easily embedded into 3D CNN-based action recognition models. Extensive experiments on the challenging HMDB-51 and UCF-101 datasets showed that our proposed module achieves state-of-the-art performance and can significantly improve the recognition accuracy of 3D CNN-based action recognition methods.</p>
<p><strong>Download:</strong> <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10489-021-02195-8">[官方链接]</a> <a target="_blank" rel="noopener" href="https://github.com/sues-vision/pre-print-papers">[preprint版本]</a></p>
<p><strong>Keywords:</strong> Effective attention module, 3D CNN, Deep learning, Action recognition.</p>
<p><strong>Photos:</strong> </p>
<img src="/2021/03/01/paper-2021-AI/fig3.png" class>
  </div>
  
  <p class="article-meta">
    <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2021/03/01/paper-2021-AI/" class="article-date">
	  <time datetime="2021-03-01T07:05:22.000Z" itemprop="datePublished">3月 1</time>
	</a>
</span>
    
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/AcademicActivities/">AcademicActivities</a>
  </span>

    
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E4%B8%89%E7%BB%B4-3D/" rel="tag">三维 3D</a>, <a class="article-tag-link-link" href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB-Action-Recognition/" rel="tag">动作识别 Action Recognition</a>, <a class="article-tag-link-link" href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Attention/" rel="tag">注意力机制 Attention</a>, <a class="article-tag-link-link" href="/tags/%E8%AE%BA%E6%96%87-Article/" rel="tag">论文 Article</a>
  </span>


    <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2021/03/01/paper-2021-AI/#comments" class="article-comment-link">评论</a></span>
    
  </p>
</article>

    
  </div>

  
    
    <nav class="bar bar-footer clearfix" data-stick-bottom>
        <div class="bar-inner">
        <ul class="pager pull-left">
            
            <li class="prev">
                <a href="/">
                    <i class="icon icon-angle-left"></i>
                    上一页
                </a>
            </li>
            
            
            <li class="next">
                <a href="/page/3/">
                    下一页
                    <i class="icon icon-angle-right"></i>
                </a>
            </li>
            
        </ul>
            <div class="total-article bar-right">Page 2 of 4</div>
        </div>
    </nav>
    
 
</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   









</body>
</html>